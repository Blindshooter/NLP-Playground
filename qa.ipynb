{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = \"Jack liked Maria but she didn't like him back because she dreamed about Mexico in 1996\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tree('S', [Tree('PERSON', [('Jack', 'NNP')]), ('liked', 'VBD'), Tree('PERSON', [('Maria', 'NNP')]), ('but', 'CC'), ('she', 'PRP'), ('did', 'VBD'), (\"n't\", 'RB'), ('like', 'VB'), ('him', 'PRP'), ('back', 'RB'), ('because', 'IN'), ('she', 'PRP'), ('dreamed', 'VBD'), ('about', 'IN'), Tree('GPE', [('Mexico', 'NNP')]), ('in', 'IN'), ('1996', 'CD')])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ne_chunk(pos_tag(word_tokenize(sentence)))#.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.en import English\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = nlp(u\"Jack liked Maria but she didn't like him back because she dreamed about Mexico in 1996\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Jack, u'NNP'),\n",
       " (liked, u'VBD'),\n",
       " (Maria, u'NNP'),\n",
       " (but, u'CC'),\n",
       " (she, u'PRP'),\n",
       " (did, u'VBD'),\n",
       " (n't, u'RB'),\n",
       " (like, u'VB'),\n",
       " (him, u'PRP'),\n",
       " (back, u'RB'),\n",
       " (because, u'IN'),\n",
       " (she, u'PRP'),\n",
       " (dreamed, u'VBD'),\n",
       " (about, u'IN'),\n",
       " (Mexico, u'NNP'),\n",
       " (in, u'IN'),\n",
       " (1996, u'CD')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = []\n",
    "for t in x:\n",
    "    tags.append((t, t.tag_))\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'Jack', u'PERSON', u'jack', u'nsubj')\n",
      "(u'liked', u'', u'like', u'ROOT')\n",
      "(u'Maria', u'PERSON', u'maria', u'dobj')\n",
      "(u'but', u'', u'but', u'cc')\n",
      "(u'she', u'', u'she', u'nsubj')\n",
      "(u'did', u'', u'do', u'aux')\n",
      "(u\"n't\", u'', u'not', u'neg')\n",
      "(u'like', u'', u'like', u'conj')\n",
      "(u'him', u'', u'him', u'dobj')\n",
      "(u'back', u'', u'back', u'advmod')\n",
      "(u'because', u'', u'because', u'mark')\n",
      "(u'she', u'', u'she', u'nsubj')\n",
      "(u'dreamed', u'', u'dream', u'advcl')\n",
      "(u'about', u'', u'about', u'prep')\n",
      "(u'Mexico', u'GPE', u'mexico', u'pobj')\n",
      "(u'in', u'', u'in', u'prep')\n",
      "(u'1996', u'DATE', u'1996', u'pobj')\n"
     ]
    }
   ],
   "source": [
    "data = nlp(unicode(sentence))\n",
    "for token in data:\n",
    "    print (token.orth_, token.ent_type_, token.lemma_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'In', u'', u'in', u'prep', become, 8)\n",
      "(u'the', u'', u'the', u'det', middle, 2)\n",
      "(u'middle', u'', u'middle', u'pobj', In, 0)\n",
      "(u'of', u'', u'of', u'prep', middle, 2)\n",
      "(u'1990s', u'', u'1990s', u'pobj', of, 3)\n",
      "(u'Ivan', u'PERSON', u'ivan', u'nsubj', become, 8)\n",
      "(u'had', u'', u'have', u'aux', become, 8)\n",
      "(u'already', u'', u'already', u'advmod', become, 8)\n",
      "(u'become', u'', u'become', u'ROOT', become, 8)\n",
      "(u'accustomed', u'', u'accustomed', u'acomp', become, 8)\n",
      "(u'to', u'', u'to', u'prep', accustomed, 9)\n",
      "(u'the', u'GPE', u'the', u'det', system, 15)\n",
      "(u'Soviet', u'GPE', u'soviet', u'compound', Union, 13)\n",
      "(u'Union', u'GPE', u'union', u'compound', system, 15)\n",
      "(u'law', u'', u'law', u'compound', system, 15)\n",
      "(u'system', u'', u'system', u'pobj', to, 10)\n",
      "(u'as', u'', u'as', u'mark', forget, 19)\n",
      "(u'they', u'', u'they', u'nsubj', forget, 19)\n",
      "(u'never', u'', u'never', u'neg', forget, 19)\n",
      "(u'forget', u'', u'forget', u'advcl', become, 8)\n",
      "(u'a', u'', u'a', u'det', chance, 21)\n",
      "(u'chance', u'', u'chance', u'dobj', forget, 19)\n",
      "(u'to', u'', u'to', u'aux', sue, 23)\n",
      "(u'sue', u'', u'sue', u'acl', chance, 21)\n",
      "(u'him', u'', u'him', u'dobj', sue, 23)\n"
     ]
    }
   ],
   "source": [
    "sentence = \"In the middle of 1990s Ivan had already become accustomed to the Soviet Union law system as they never forget a chance to sue him\"\n",
    "data = nlp(unicode(sentence))\n",
    "for token in data:\n",
    "    print (token.orth_, token.ent_type_, token.lemma_, token.dep_, token.head, token.head.i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'Here', u'', u'ADV', u'advmod')\n",
      "(u'Jimmy', u'PERSON', u'PROPN', u'nsubj')\n",
      "(u'is', u'', u'VERB', u'aux')\n",
      "(u'using', u'', u'VERB', u'ROOT')\n",
      "(u'Python', u'PRODUCT', u'PROPN', u'dobj')\n",
      "(u'to', u'', u'PART', u'aux')\n",
      "(u'see', u'', u'VERB', u'xcomp')\n",
      "(u'behavior', u'', u'NOUN', u'dobj')\n",
      "(u'of', u'', u'ADP', u'prep')\n",
      "(u'our', u'', u'ADJ', u'poss')\n",
      "(u'users', u'', u'NOUN', u'pobj')\n"
     ]
    }
   ],
   "source": [
    "s1 = unicode(\"Here Jimmy is using Python to see behavior of our users\")\n",
    "data = nlp(s1)\n",
    "for token in data:\n",
    "    print (token.orth_, token.ent_type_, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_continuous_chunks(text):\n",
    "    chunked = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "    prev = None\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "    for i in chunked:\n",
    "            if type(i) == Tree:\n",
    "                    current_chunk.append(\" \".join([token for token, pos in i.leaves()]))\n",
    "            elif current_chunk:\n",
    "                    named_entity = \" \".join(current_chunk)\n",
    "                    if named_entity not in continuous_chunk:\n",
    "                            continuous_chunk.append(named_entity)\n",
    "                            current_chunk = []\n",
    "            else:\n",
    "                    continue\n",
    "    return continuous_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Jimmy', u'Python']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_continuous_chunks(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'Maybe', u'', u'ADV', u'advmod')\n",
      "(u'Vasyl', u'PERSON', u'PROPN', u'nsubj')\n",
      "(u'was', u'', u'VERB', u'aux')\n",
      "(u'playing', u'', u'VERB', u'ROOT')\n",
      "(u'baseball', u'', u'NOUN', u'dobj')\n",
      "(u'in', u'', u'ADP', u'prep')\n",
      "(u'New', u'GPE', u'PROPN', u'compound')\n",
      "(u'York', u'GPE', u'PROPN', u'pobj')\n",
      "(u'last', u'DATE', u'ADJ', u'amod')\n",
      "(u'summer', u'DATE', u'NOUN', u'npadvmod')\n",
      "(u'with', u'', u'ADP', u'prep')\n",
      "(u'his', u'', u'ADJ', u'poss')\n",
      "(u'dog', u'', u'NOUN', u'pobj')\n"
     ]
    }
   ],
   "source": [
    "s2 = unicode(\"Maybe Vasyl was playing baseball in New York last summer with his dog\")\n",
    "data = nlp(s2)\n",
    "for token in data:\n",
    "    print (token.orth_, token.ent_type_, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Have you heard that he is playing baseball in New York?'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def did_you_know(sentence):\n",
    "    check = nlp(unicode(\"Maybe \"+sentence))\n",
    "    check_tokens = [t.ent_type_ for t in check]\n",
    "    ent = check_tokens[1]\n",
    "    if ent=='':\n",
    "        sentence = sentence[0].lower()+sentence[1:]\n",
    "    question_begin = ['Did you know that ', 'Are you familiar with the fact that ', 'Have you heard that ']\n",
    "    question = random.choice(question_begin)\n",
    "    #data = nlp(question)\n",
    "    #tokens = [t.orth_ for t in data]\n",
    "    return question+sentence+'?'\n",
    "did_you_know('He is playing baseball in New York')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#http://homepages.inf.ed.ac.uk/mroth/demo.html\n",
    "def form_question(start, grouping, i):\n",
    "    x = grouping[:i]+grouping[i+1:]\n",
    "    x1 = [x[1] for x in x]\n",
    "    x2 = \" \".join(x1)\n",
    "    return start+\" \"+x2+\"?\"\n",
    "\n",
    "\n",
    "\n",
    "def squestion(sentence):\n",
    "    check = nlp(unicode(\"Maybe \"+sentence))\n",
    "    check_tokens = [t.ent_type_ for t in check]\n",
    "    ent = check_tokens[1]\n",
    "    if ent=='':\n",
    "        sentenceq = sentence[0].lower()+sentence[1:]\n",
    "    subj = []\n",
    "    mverb = []\n",
    "    chunks = []\n",
    "    for i in xrange(len(check[1:])):\n",
    "    #for token in check[1:]:\n",
    "        token = check[i+1]\n",
    "        if token.dep_=='nsubj':\n",
    "            subj=[i, token.orth_]\n",
    "        if token.ent_type_=='':\n",
    "            if token.pos_=='VERB' and token.dep_=='ROOT' and check[i].dep_!='aux':\n",
    "                chunks.append(['VERB-aux', 'did'])\n",
    "                chunks.append([token.pos_, token.lemma_])\n",
    "                mverb=['did', token.orth_]\n",
    "            elif token.pos_=='VERB' and token.dep_=='ROOT' and check[i].dep_=='aux':\n",
    "                chunks.append([token.pos_, token.orth_])\n",
    "                mverb=[check[i].orth_, token.orth_]\n",
    "                \n",
    "            #elif token.pos_=='VERB' and token.dep_=='aux':\n",
    "                #chunks.append([token.pos_+'-aux', token.orth_, ])\n",
    "            else:\n",
    "                chunks.append([token.pos_, token.orth_])\n",
    "        else:\n",
    "            chunks.append([token.ent_type_, token.orth_])\n",
    "    \n",
    "    \n",
    "    grouping = [chunks[0]]\n",
    "    for item in chunks[1:]:\n",
    "        #print (item[0], grouping[-1][0])\n",
    "        if item[0]==grouping[-1][0]:\n",
    "            grouping[-1][1]+=' '+item[1]\n",
    "        else:\n",
    "            grouping.append(item)\n",
    "    quest_begin = {'PERSON':'Who', 'GPE':'Where', 'DATE':'When', 'NOUN':'What'}\n",
    "    \n",
    "    #for key in quest_begin.keys():\n",
    "    #for item in grouping:\n",
    "    qlist = []\n",
    "    for i in xrange(len(grouping)):\n",
    "        item = grouping[i]\n",
    "        if item[0] in quest_begin.keys():\n",
    "            start = quest_begin[item[0]]\n",
    "            qlist.append(form_question(start, grouping, i))\n",
    "    \n",
    "    return qlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Who was programming in python in New York last summer?',\n",
       " u'What Igor Lushchyk was programming in in New York last summer?',\n",
       " u'Where Igor Lushchyk was programming in python in last summer?',\n",
       " u'When Igor Lushchyk was programming in python in New York?']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squestion('Igor Lushchyk was programming in python in New York last summer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Who is showing us a demo in Lviv right now?',\n",
       " u'What Sergiy is showing us a in Lviv right now?',\n",
       " u'Where Sergiy is showing us a demo in right now?']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squestion('Sergiy is showing us a demo in Lviv right now')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
