{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import random\n",
    "import spacy\n",
    "from spacy.en import English\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over the past few years, we have made dramatic breakthroughs.Machine learning is now capable of far, far more complex tasks.\n",
      "Now, given the right data, machines are going to outperform humans at tasks like this.\n",
      "The fundamental limitations of machine learning is that it needs to learn from large volumes of past data.Now, humans don't.\n",
      "Machines cannot compete with us when it comes to tackling novel situations, and this puts a fundamental limit on the human tasks that machines will automate.\n",
      "The future state of any single job lies in the answer to a single question: To what extent is that job reducible to frequent, high-volume tasks, and to what extent does it involve tackling novel situations?\n"
     ]
    }
   ],
   "source": [
    "text = '''So this is my niece. Her name is Yahli. She is nine months old. Her mum is a doctor, and her dad is a lawyer. By the time Yahli goes to college, the jobs her parents do are going to look dramatically different.\n",
    "In 2013, researchers at Oxford University did a study on the future of work. They concluded that almost one in every two jobs have a high risk of being automated by machines. Machine learning is the technology that's responsible for most of this disruption. It's the most powerful branch of artificial intelligence. It allows machines to learn from data and mimic some of the things that humans can do. My company, Kaggle, operates on the cutting edge of machine learning. We bring together hundreds of thousands of experts to solve important problems for industry and academia. This gives us a unique perspective on what machines can do, what they can't do and what jobs they might automate or threaten.\n",
    "Machine learning started making its way into industry in the early '90s. It started with relatively simple tasks. It started with things like assessing credit risk from loan applications, sorting the mail by reading handwritten characters from zip codes. Over the past few years, we have made dramatic breakthroughs.Machine learning is now capable of far, far more complex tasks. In 2012, Kaggle challenged its community to build an algorithm that could grade high-school essays. The winning algorithms were able to match the grades given by human teachers. Last year, we issued an even more difficult challenge. Can you take images of the eye and diagnose an eye disease called diabetic retinopathy? Again, the winning algorithms were able to match the diagnoses given by human ophthalmologists.\n",
    "Now, given the right data, machines are going to outperform humans at tasks like this. A teacher might read 10,000 essays over a 40-year career. An ophthalmologist might see 50,000 eyes. A machine can read millions of essays or see millions of eyes within minutes. We have no chance of competing against machines on frequent, high-volume tasks.\n",
    "But there are things we can do that machines can't do. Where machines have made very little progress is in tackling novel situations. They can't handle things they haven't seen many times before. The fundamental limitations of machine learning is that it needs to learn from large volumes of past data.Now, humans don't. We have the ability to connect seemingly disparate threads to solve problems we've never seen before.\n",
    "Percy Spencer was a physicist working on radar during World War II, when he noticed the magnetron was melting his chocolate bar. He was able to connect his understanding of electromagnetic radiation with his knowledge of cooking in order to invent -- any guesses? -- the microwave oven.\n",
    "Now, this is a particularly remarkable example of creativity. But this sort of cross-pollination happens for each of us in small ways thousands of times per day. Machines cannot compete with us when it comes to tackling novel situations, and this puts a fundamental limit on the human tasks that machines will automate.\n",
    "So what does this mean for the future of work? The future state of any single job lies in the answer to a single question: To what extent is that job reducible to frequent, high-volume tasks, and to what extent does it involve tackling novel situations? On frequent, high-volume tasks, machines are getting smarter and smarter. Today they grade essays. They diagnose certain diseases. Over coming years, they're going to conduct our audits, and they're going to read boilerplate from legal contracts. Accountants and lawyers are still needed. They're going to be needed for complex tax structuring, for pathbreaking litigation. But machines will shrink their ranks and make these jobs harder to come by.\n",
    "Now, as mentioned, machines are not making progress on novel situations. The copy behind a marketing campaign needs to grab consumers' attention. It has to stand out from the crowd. Business strategy means finding gaps in the market, things that nobody else is doing. It will be humans that are creating the copy behind our marketing campaigns, and it will be humans that are developing our business strategy.\n",
    "So Yahli, whatever you decide to do, let every day bring you a new challenge. If it does, then you will stay ahead of the machines.\n",
    "Thank you.\n",
    "'''\n",
    "summary = gensim.summarization.summarize(text, 0.1)\n",
    "print summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I study planetary habitability as influenced by stars with the hopes of finding the places in the universe where we might discover life beyond our own planet.\n",
      "Mars is small and rocky, and though it's a bit far from the Sun, it might be considered a potentially habitable world if found by a mission like Kepler.\n",
      "But the goals of interplanetary exploration and planetary preservation are not opposed to one another.No, they're in fact two sides of the same goal: to understand, preserve and improve life into the future.The extreme environments of our own world are alien vistas.\n"
     ]
    }
   ],
   "source": [
    "text2 = '''We're at a tipping point in human history, a species poised between gaining the stars and losing the planet we call home.\n",
    "Even in just the past few years, we've greatly expanded our knowledge of how Earth fits within the context of our universe. NASA's Kepler mission has discovered thousands of potential planets around other stars, indicating that Earth is but one of billions of planets in our galaxy. Kepler is a space telescopethat measures the subtle dimming of stars as planets pass in front of them, blocking just a little bit of that light from reaching us. Kepler's data reveals planets' sizes as well as their distance from their parent star.Together, this helps us understand whether these planets are small and rocky, like the terrestrial planets in our own Solar System, and also how much light they receive from their parent sun. In turn, this provides clues as to whether these planets that we discover might be habitable or not.\n",
    "Unfortunately, at the same time as we're discovering this treasure trove of potentially habitable worlds,our own planet is sagging under the weight of humanity. 2014 was the hottest year on record. Glaciers and sea ice that have been with us for millennia are now disappearing in a matter of decades. These planetary-scale environmental changes that we have set in motion are rapidly outpacing our ability to alter their course.\n",
    "But I'm not a climate scientist, I'm an astronomer. I study planetary habitability as influenced by stars with the hopes of finding the places in the universe where we might discover life beyond our own planet. You could say that I look for choice alien real estate.\n",
    "Now, as somebody who is deeply embedded in the search for life in the universe, I can tell you that the more you look for planets like Earth, the more you appreciate our own planet itself. Each one of these new worlds invites a comparison between the newly discovered planet and the planets we know best:those of our own Solar System.\n",
    "Consider our neighbor, Mars. Mars is small and rocky, and though it's a bit far from the Sun, it might be considered a potentially habitable world if found by a mission like Kepler. Indeed, it's possible that Mars was habitable in the past, and in part, this is why we study Mars so much. Our rovers, like Curiosity, crawl across its surface, scratching for clues as to the origins of life as we know it. Orbiters like the MAVEN mission sample the Martian atmosphere, trying to understand how Mars might have lost its past habitability. Private spaceflight companies now offer not just a short trip to near space but the tantalizing possibility of living our lives on Mars.\n",
    "But though these Martian vistas resemble the deserts of our own home world, places that are tied in our imagination to ideas about pioneering and frontiers, compared to Earth Mars is a pretty terrible place to live. Consider the extent to which we have not colonized the deserts of our own planet, places that are lush by comparison with Mars. Even in the driest, highest places on Earth, the air is sweet and thick with oxygen exhaled from thousands of miles away by our rainforests.\n",
    "I worry -- I worry that this excitement about colonizing Mars and other planets carries with it a long, dark shadow: the implication and belief by some that Mars will be there to save us from the self-inflicted destruction of the only truly habitable planet we know of, the Earth. As much as I love interplanetary exploration, I deeply disagree with this idea. There are many excellent reasons to go to Mars, but for anyone to tell you that Mars will be there to back up humanity is like the captain of the Titanic telling you that the real party is happening later on the lifeboats. (\n",
    "Thank you.\n",
    "But the goals of interplanetary exploration and planetary preservation are not opposed to one another.No, they're in fact two sides of the same goal: to understand, preserve and improve life into the future.The extreme environments of our own world are alien vistas. They're just closer to home. If we can understand how to create and maintain habitable spaces out of hostile, inhospitable spaces here on Earth, perhaps we can meet the needs of both preserving our own environment and moving beyond it.\n",
    "I leave you with a final thought experiment: Fermi's paradox. Many years ago, the physicist Enrico Fermi asked that, given the fact that our universe has been around for a very long time and we expect that there are many planets within it, we should have found evidence for alien life by now. So where are they? Well, one possible solution to Fermi's paradox is that, as civilizations become technologically advanced enough to consider living amongst the stars, they lose sight of how important it is to safeguard the home worlds that fostered that advancement to begin with. It is hubris to believe that interplanetary colonization alone will save us from ourselves, but planetary preservation and interplanetary exploration can work together.\n",
    "If we truly believe in our ability to bend the hostile environments of Mars for human habitation, then we should be able to surmount the far easier task of preserving the habitability of the Earth.\n",
    "Thank you.\n",
    "'''\n",
    "summary2 = gensim.summarization.summarize(text2, 0.1)\n",
    "print summary2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nouns_list(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    sent_list = []\n",
    "    for sentence in sentences:\n",
    "        #data = nlp(unicode(\"Maybe \"+ sentence))\n",
    "        data = nlp(unicode(sentence))\n",
    "        x = []\n",
    "        for token in data:\n",
    "            x.append((token.orth_, token.ent_type_, token.pos_, token.dep_))\n",
    "        sent_list.append(x)\n",
    "    return sent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(u'Over', u'', u'ADP', u'prep'),\n",
       "  (u'the', u'DATE', u'DET', u'det'),\n",
       "  (u'past', u'DATE', u'ADJ', u'amod'),\n",
       "  (u'few', u'DATE', u'ADJ', u'amod'),\n",
       "  (u'years', u'DATE', u'NOUN', u'pobj'),\n",
       "  (u',', u'', u'PUNCT', u'punct'),\n",
       "  (u'we', u'', u'PRON', u'nsubj'),\n",
       "  (u'have', u'', u'VERB', u'aux'),\n",
       "  (u'made', u'', u'VERB', u'ROOT'),\n",
       "  (u'dramatic', u'', u'ADJ', u'amod'),\n",
       "  (u'breakthroughs', u'', u'NOUN', u'dobj'),\n",
       "  (u'.', u'', u'PUNCT', u'punct'),\n",
       "  (u'Machine', u'', u'NOUN', u'compound'),\n",
       "  (u'learning', u'', u'NOUN', u'nsubj'),\n",
       "  (u'is', u'', u'VERB', u'ROOT'),\n",
       "  (u'now', u'', u'ADV', u'advmod'),\n",
       "  (u'capable', u'', u'ADJ', u'acomp'),\n",
       "  (u'of', u'', u'ADP', u'prep'),\n",
       "  (u'far', u'', u'ADV', u'amod'),\n",
       "  (u',', u'', u'PUNCT', u'punct'),\n",
       "  (u'far', u'', u'ADV', u'advmod'),\n",
       "  (u'more', u'', u'ADV', u'advmod'),\n",
       "  (u'complex', u'', u'ADJ', u'amod'),\n",
       "  (u'tasks', u'', u'NOUN', u'pobj'),\n",
       "  (u'.', u'', u'PUNCT', u'punct')],\n",
       " [(u'Now', u'', u'ADV', u'advmod'),\n",
       "  (u',', u'', u'PUNCT', u'punct'),\n",
       "  (u'given', u'', u'VERB', u'prep'),\n",
       "  (u'the', u'', u'DET', u'det'),\n",
       "  (u'right', u'', u'ADJ', u'amod'),\n",
       "  (u'data', u'', u'NOUN', u'pobj'),\n",
       "  (u',', u'', u'PUNCT', u'punct'),\n",
       "  (u'machines', u'', u'NOUN', u'nsubj'),\n",
       "  (u'are', u'', u'VERB', u'aux'),\n",
       "  (u'going', u'', u'VERB', u'ROOT'),\n",
       "  (u'to', u'', u'PART', u'aux'),\n",
       "  (u'outperform', u'', u'VERB', u'xcomp'),\n",
       "  (u'humans', u'', u'NOUN', u'dobj'),\n",
       "  (u'at', u'', u'ADP', u'prep'),\n",
       "  (u'tasks', u'', u'NOUN', u'pobj'),\n",
       "  (u'like', u'', u'ADP', u'prep'),\n",
       "  (u'this', u'', u'DET', u'pobj'),\n",
       "  (u'.', u'', u'PUNCT', u'punct')],\n",
       " [(u'The', u'', u'DET', u'det'),\n",
       "  (u'fundamental', u'', u'ADJ', u'amod'),\n",
       "  (u'limitations', u'', u'NOUN', u'nsubj'),\n",
       "  (u'of', u'', u'ADP', u'prep'),\n",
       "  (u'machine', u'', u'NOUN', u'compound'),\n",
       "  (u'learning', u'', u'NOUN', u'pobj'),\n",
       "  (u'is', u'', u'VERB', u'ROOT'),\n",
       "  (u'that', u'', u'ADP', u'mark'),\n",
       "  (u'it', u'', u'PRON', u'nsubj'),\n",
       "  (u'needs', u'', u'VERB', u'ccomp'),\n",
       "  (u'to', u'', u'PART', u'aux'),\n",
       "  (u'learn', u'', u'VERB', u'xcomp'),\n",
       "  (u'from', u'', u'ADP', u'prep'),\n",
       "  (u'large', u'', u'ADJ', u'amod'),\n",
       "  (u'volumes', u'', u'NOUN', u'pobj'),\n",
       "  (u'of', u'', u'ADP', u'prep'),\n",
       "  (u'past', u'', u'ADJ', u'amod'),\n",
       "  (u'data', u'', u'NOUN', u'pobj'),\n",
       "  (u'.', u'', u'PUNCT', u'punct'),\n",
       "  (u'Now', u'', u'ADV', u'advmod'),\n",
       "  (u',', u'', u'PUNCT', u'punct'),\n",
       "  (u'humans', u'', u'NOUN', u'nsubj'),\n",
       "  (u'do', u'', u'VERB', u'ROOT'),\n",
       "  (u\"n't\", u'', u'ADV', u'neg'),\n",
       "  (u'.', u'', u'PUNCT', u'punct')],\n",
       " [(u'Machines', u'', u'PROPN', u'nsubj'),\n",
       "  (u'can', u'', u'VERB', u'aux'),\n",
       "  (u'not', u'', u'ADV', u'neg'),\n",
       "  (u'compete', u'', u'VERB', u'ROOT'),\n",
       "  (u'with', u'', u'ADP', u'prep'),\n",
       "  (u'us', u'', u'PRON', u'pobj'),\n",
       "  (u'when', u'', u'ADV', u'advmod'),\n",
       "  (u'it', u'', u'PRON', u'nsubj'),\n",
       "  (u'comes', u'', u'VERB', u'advcl'),\n",
       "  (u'to', u'', u'ADP', u'prep'),\n",
       "  (u'tackling', u'', u'VERB', u'pcomp'),\n",
       "  (u'novel', u'', u'ADJ', u'amod'),\n",
       "  (u'situations', u'', u'NOUN', u'dobj'),\n",
       "  (u',', u'', u'PUNCT', u'punct'),\n",
       "  (u'and', u'', u'CONJ', u'cc'),\n",
       "  (u'this', u'', u'DET', u'nsubj'),\n",
       "  (u'puts', u'', u'VERB', u'conj'),\n",
       "  (u'a', u'', u'DET', u'det'),\n",
       "  (u'fundamental', u'', u'ADJ', u'amod'),\n",
       "  (u'limit', u'', u'NOUN', u'dobj'),\n",
       "  (u'on', u'', u'ADP', u'prep'),\n",
       "  (u'the', u'', u'DET', u'det'),\n",
       "  (u'human', u'', u'ADJ', u'amod'),\n",
       "  (u'tasks', u'', u'NOUN', u'pobj'),\n",
       "  (u'that', u'', u'ADJ', u'dobj'),\n",
       "  (u'machines', u'', u'NOUN', u'nsubj'),\n",
       "  (u'will', u'', u'VERB', u'aux'),\n",
       "  (u'automate', u'', u'VERB', u'relcl'),\n",
       "  (u'.', u'', u'PUNCT', u'punct')],\n",
       " [(u'The', u'', u'DET', u'det'),\n",
       "  (u'future', u'', u'ADJ', u'amod'),\n",
       "  (u'state', u'', u'NOUN', u'nsubj'),\n",
       "  (u'of', u'', u'ADP', u'prep'),\n",
       "  (u'any', u'', u'DET', u'det'),\n",
       "  (u'single', u'', u'ADJ', u'amod'),\n",
       "  (u'job', u'', u'NOUN', u'pobj'),\n",
       "  (u'lies', u'', u'VERB', u'ROOT'),\n",
       "  (u'in', u'', u'ADP', u'prep'),\n",
       "  (u'the', u'', u'DET', u'det'),\n",
       "  (u'answer', u'', u'NOUN', u'pobj'),\n",
       "  (u'to', u'', u'ADP', u'prep'),\n",
       "  (u'a', u'', u'DET', u'det'),\n",
       "  (u'single', u'', u'ADJ', u'amod'),\n",
       "  (u'question', u'', u'NOUN', u'pobj'),\n",
       "  (u':', u'', u'PUNCT', u'punct'),\n",
       "  (u'To', u'', u'PART', u'prep'),\n",
       "  (u'what', u'', u'ADJ', u'det'),\n",
       "  (u'extent', u'', u'NOUN', u'pcomp'),\n",
       "  (u'is', u'', u'VERB', u'acl'),\n",
       "  (u'that', u'', u'ADP', u'mark'),\n",
       "  (u'job', u'', u'NOUN', u'compound'),\n",
       "  (u'reducible', u'', u'ADJ', u'nsubj'),\n",
       "  (u'to', u'', u'ADP', u'prep'),\n",
       "  (u'frequent', u'', u'ADJ', u'amod'),\n",
       "  (u',', u'', u'PUNCT', u'punct'),\n",
       "  (u'high', u'', u'ADJ', u'amod'),\n",
       "  (u'-', u'', u'PUNCT', u'punct'),\n",
       "  (u'volume', u'', u'NOUN', u'compound'),\n",
       "  (u'tasks', u'', u'NOUN', u'pobj'),\n",
       "  (u',', u'', u'PUNCT', u'punct'),\n",
       "  (u'and', u'', u'CONJ', u'cc'),\n",
       "  (u'to', u'', u'ADP', u'prep'),\n",
       "  (u'what', u'', u'ADJ', u'det'),\n",
       "  (u'extent', u'', u'NOUN', u'pcomp'),\n",
       "  (u'does', u'', u'VERB', u'aux'),\n",
       "  (u'it', u'', u'PRON', u'nsubj'),\n",
       "  (u'involve', u'', u'VERB', u'conj'),\n",
       "  (u'tackling', u'', u'VERB', u'xcomp'),\n",
       "  (u'novel', u'', u'ADJ', u'amod'),\n",
       "  (u'situations', u'', u'NOUN', u'dobj'),\n",
       "  (u'?', u'', u'PUNCT', u'punct')]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns_list(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'Maybe', u'', u'ADV', u'advmod')\n",
      "(u'Vasyl', u'PERSON', u'PROPN', u'nsubj')\n",
      "(u'was', u'', u'VERB', u'aux')\n",
      "(u'playing', u'', u'VERB', u'ROOT')\n",
      "(u'baseball', u'', u'NOUN', u'dobj')\n",
      "(u'in', u'', u'ADP', u'prep')\n",
      "(u'New', u'GPE', u'PROPN', u'compound')\n",
      "(u'York', u'GPE', u'PROPN', u'pobj')\n",
      "(u'last', u'DATE', u'ADJ', u'amod')\n",
      "(u'summer', u'DATE', u'NOUN', u'npadvmod')\n",
      "(u'with', u'', u'ADP', u'prep')\n",
      "(u'his', u'', u'ADJ', u'poss')\n",
      "(u'dog', u'', u'NOUN', u'pobj')\n"
     ]
    }
   ],
   "source": [
    "s2 = unicode(\"Maybe Vasyl was playing baseball in New York last summer with his dog\")\n",
    "data = nlp(s2)\n",
    "for token in data:\n",
    "    print (token.orth_, token.ent_type_, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
